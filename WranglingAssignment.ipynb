{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annikarichardson2024/Wrangling/blob/main/Copy_of_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling\n",
        "## `! git clone https://github.com/DS3001/wrangling`\n",
        "## Do Q2, and one of Q1 or Q3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Not enough effort has been put into making data cleaning an easy and efficient process. The abstract highlights that tidy datasets have a specific structure that makes them incredibly easy to use. It leverages a small set of tools to make a huge impact on data sets.\n",
        "\n",
        "Q2. The tidy data standard is intended to easily strucutre data sets to make them effective and efficient for future analysis. It is a standardized process of organizing values and using tools to save time and labor when completeing the process.\n",
        "\n",
        "Q3. The first sentence is attempting to clarify that easy messy data set comes with its own unique set of problems; however, when a data set is tidy, it shares a unique set of principles and common themes as all other tidy data sets. The second phrase idenitifies that it is often east to identify certain aspects of a data set and catagorize them but what many people struggle to understand is the meaning and casue behind each aspect of the dataset.\n",
        "\n",
        "Q4. Values for a group of information that makes up a dataset. And each value belongs to both an observation and variable. A variable is a set of all values measuring the same attribute. An observation consists of values all from the same unit of measurement across attributes.\n",
        "\n",
        "Q5. Tidy data is defined as a standard way of mapping a dataset's meaning to its structure. Tidy data has each variable forming a column, each observation forming a row, and each observational unit forming a table.\n",
        "\n",
        "Q6. The five most common problems with data sets are:\n",
        "    - Column headers are values when they should be variable names instead\n",
        "    - Multiple varaibles are stored in one column when it should just one variable per column\n",
        "    - Variables are stored in both rows and columns. They should only be in columns\n",
        "    - Multiple types of observational units are stored in the same table. There should be seperate data sets for seperate groups.\n",
        "    - A single observational unit is stored in multiple tables. This is repetitive and confusing for those using the data.\n",
        "     In Table 4, since income is the variable mainly being measured here, the columns need to be based on the the names of variables, rather than the values of the variable. Melting a dataset involves converting column variables into rows.\n",
        "\n",
        "  Q7. Table 11 has different days as columns when they are values and not variables. Table 12 combines the values into a single variable called date. However, the element column contains measurements of the same unit and not values. Table 12(b) has all columns that are all different attributes.\n",
        "\n",
        "  Q8. The chicken and the egg problem is that data is only as useful as the tools that work with it and tidy tools are linked to the tidy data. Breaking away from this involves creating larger networks of knowledge on how to use the tools available. Human factors and user-centered design can contribute to forming this network."
      ],
      "metadata": {
        "id": "09JCAMiurFZe"
      },
      "id": "09JCAMiurFZe"
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "HGRH5_Og3PdJ"
      },
      "id": "HGRH5_Og3PdJ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PART A\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/DS3001/wrangling/main/assignment/data/airbnb_hw.csv', low_memory=False)\n",
        "\n",
        "price = df['Price']\n",
        "price.unique()\n",
        "\n",
        "commas = price.str.replace(',','')\n",
        "#EXPLANATION: removed the commas in 4 digit numbers so they can be converted to numeric instead of strings\n",
        "price = pd.to_numeric(price,errors='coerce')\n",
        "#EXPLANATION: converted the strings to numeric\n",
        "\n",
        "print( price.unique() , '\\n')\n",
        "\n",
        "totmis = sum( price.isnull() )\n",
        "print( 'Total nulls: ', totmis )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i6Dw8TbSSFy",
        "outputId": "5a8cef96-8dcc-40a4-f8df-6bdc3b107e6e"
      },
      "id": "3i6Dw8TbSSFy",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[145.  37.  28. 199. 549. 149. 250.  90. 270. 290. 170.  59.  49.  68.\n",
            " 285.  75. 100. 150. 700. 125. 175.  40.  89.  95.  99. 499. 120.  79.\n",
            " 110. 180. 143. 230. 350. 135.  85.  60.  70.  55.  44. 200. 165. 115.\n",
            "  74.  84. 129.  50. 185.  80. 190. 140.  45.  65. 225. 600. 109.  nan\n",
            "  73. 240.  72. 105. 155. 160.  42. 132. 117. 295. 280. 159. 107.  69.\n",
            " 239. 220. 399. 130. 375. 585. 275. 139. 260.  35. 133. 300. 289. 179.\n",
            "  98. 195.  29.  27.  39. 249. 192. 142. 169. 131. 138. 113. 122. 329.\n",
            " 101. 475. 238. 272. 308. 126. 235. 315. 248. 128.  56. 207. 450. 215.\n",
            " 210. 385. 445. 136. 247. 118.  77.  76.  92. 198. 205. 299. 222. 245.\n",
            " 104. 153. 349. 114. 320. 292. 226. 420. 500. 325. 307.  78. 265. 108.\n",
            " 123. 189.  32.  58.  86. 219. 800. 335.  63. 229. 425.  67.  87. 158.\n",
            " 650. 234. 310. 695. 400. 166. 119.  62. 168. 340. 479.  43. 395. 144.\n",
            "  52.  47. 529. 187. 209. 233.  82. 269. 163. 172. 305. 156. 550. 435.\n",
            " 137. 124.  48. 279. 330. 134. 378.  97. 277.  64. 193. 147. 186. 264.\n",
            "  30. 112.  94. 379.  57. 415. 236. 410. 214.  88.  66.  71. 171. 157.\n",
            " 545.  83.  96.  81. 188. 380. 255. 505.  54.  33. 174.  93. 740. 640.\n",
            " 440. 599. 357. 495. 127. 178. 348. 152. 242. 183. 253. 750. 259. 365.\n",
            " 273. 197. 397. 103. 389. 355. 559.  38. 203. 999. 141. 162. 333. 698.\n",
            "  46. 360. 895.  10.  41. 206. 281. 449. 388. 212. 102. 201. 432. 675.\n",
            " 167. 390. 298. 339. 194. 302. 211. 595. 191.  53. 361. 480. 459. 997.\n",
            " 345. 216. 218. 111. 735. 276.  91. 490. 850. 398.  36. 775. 267. 625.\n",
            " 336. 176. 725. 469. 106. 460. 287. 575. 227. 263.  25. 228. 208. 177.\n",
            " 880. 148. 116. 685. 470. 217. 164.  61. 645. 699. 405. 252. 319. 268.\n",
            " 419. 343. 525. 311. 840. 154. 294. 950. 409. 184. 257. 204. 241. 412.\n",
            " 121. 288. 196. 900. 647. 524. 309. 510. 799. 383. 372. 492. 327. 656.\n",
            " 224. 173. 875. 795. 690. 146. 465. 151. 274. 429. 825. 282. 256. 620.\n",
            " 271. 161.  51. 855. 579. 430.  20. 899. 649. 485. 181. 455. 243. 342.\n",
            " 590. 560. 374. 437. 232. 359. 985.  31. 244. 254. 723. 237. 428. 370.\n",
            "  34. 580. 221. 749. 306. 202. 680. 570. 520. 223. 213. 346.  24. 286.\n",
            " 296. 266.  26. 995. 393. 182. 635. 258. 780. 589. 347. 446. 975. 323.\n",
            " 715. 461. 540. 356. 439. 384. 569.  22. 785. 626. 830. 318. 444. 321.\n",
            " 401. 888. 369. 770. 386. 366. 344. 630. 313. 597. 262. 509. 278. 312.\n",
            " 789. 422.  21. 765. 945. 326. 472. 454. 328. 396. 291.] \n",
            "\n",
            "Total nulls:  181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PART B\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/DS3001/wrangling/main/data/sharks.csv', low_memory=False)\n",
        "\n",
        "\n",
        "AttackType = df['Type']\n",
        "\n",
        "AttackType = AttackType.replace(['Unconfirmed', 'Unverified','Under investigation','Boatomg'],'Questionable')\n",
        "AttackType.value_counts()\n",
        "#EXPLANATION: combining all the values that relate to being unclear\n",
        "\n",
        "\n",
        "AttackType = AttackType.replace(['Boating', 'Watercraft'],'Boat')\n",
        "AttackType.value_counts()\n",
        "#EXPLANATION: all related to the same thing\n",
        "\n",
        "AttackType = AttackType.replace(['Invalid'],np.nan)\n",
        "AttackType.value_counts()\n",
        "#EXPLANATION: unclean value\n",
        "\n",
        "df['Type'] = AttackType\n",
        "del AttackType\n",
        "df['Type'].value_counts()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oUoF05fFq88I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4652c985-e346-433c-9f3f-b8f8cdc2bec3"
      },
      "id": "oUoF05fFq88I",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unprovoked      4716\n",
              "Provoked         593\n",
              "Boat             343\n",
              "Sea Disaster     239\n",
              "Questionable      14\n",
              "Name: Type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART C\n",
        "df = pd.read_csv('http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv', low_memory=False)\n",
        "\n",
        "\n",
        "release = df['WhetherDefendantWasReleasedPretrial']\n",
        "\n",
        "release = release.replace(9,np.nan)\n",
        "print(release.value_counts(),'\\n')\n",
        "# EXPLANATION\n",
        "#The orginal column is broken down into 3 groups being 0s, 1s, and 9s\n",
        "#However, the 9s are not clarified for being released or unreleased and have missing values so should be removed\n",
        "\n",
        "df['WhetherDefendantWasReleasedPretrial'] = release\n",
        "del release\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNnyaLNIWHR6",
        "outputId": "3c13b04c-3583-40d7-d8af-afc0efd34c75"
      },
      "id": "DNnyaLNIWHR6",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0    19154\n",
            "0.0     3801\n",
            "Name: WhetherDefendantWasReleasedPretrial, dtype: int64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART D\n",
        "\n",
        "SentenceLength = df['ImposedSentenceAllChargeInContactEvent']\n",
        "SentenceType = df['SentenceTypeAllChargesAtConvictionInContactEvent']\n",
        "\n",
        "\n",
        "SentenceLength = pd.to_numeric(SentenceLength,errors='coerce')\n",
        "\n",
        "\n",
        "SentenceLength = SentenceLength.mask( SentenceType == 4, 0)\n",
        "SentenceLength = SentenceLength.mask( SentenceType == 9, np.nan)\n",
        "# EXPLANATION\n",
        "# when type id 9, disposition record is not found so should be removed\n",
        "# when type is 4, charges were dismissed so should be in the no jail category\n",
        "\n",
        "missing = SentenceLength.isnull()\n",
        "print( pd.crosstab(missing, SentenceType), '\\n')\n",
        "\n",
        "df['ImposedSentenceAllChargeInContactEvent'] = SentenceLength\n",
        "del SentenceLength, SentenceType\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woXdjFyUXrUh",
        "outputId": "73334fda-9ef5-4c97-ad8e-e78379e66f16"
      },
      "id": "woXdjFyUXrUh",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
            "ImposedSentenceAllChargeInContactEvent                                      \n",
            "False                                             8720  4299  914  8779    0\n",
            "True                                                 0     0    0     0  274 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
